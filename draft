#!/usr/bin/env python3
"""
Update an Amazon Bedrock Prompt Management prompt DRAFT from a YAML file.

Requires:
  uv add boto3 pyyaml

Examples:
  uv run prompt_draft.py \
    --region us-east-1 \
    --prompt-name 
    --yaml ../prompts/primary.yaml \
    --model-id "amazon.nova-lite-v1:0" \
    --enable-cachepoint \
    --publish-version

Notes:
- UpdatePrompt updates the DRAFT version. (version returned will be "DRAFT")  (AWS API)  :contentReference[oaicite:8]{index=8}
"""

from __future__ import annotations

import argparse
import sys
from typing import Any, Dict, List, Optional

import boto3
import yaml


def _load_yaml(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)
    if not isinstance(data, dict):
        raise ValueError("YAML root must be a mapping/object.")
    return data


def _coerce_variables(y: Dict[str, Any]) -> List[Dict[str, str]]:
    vars_ = y.get("variables") or []
    if not isinstance(vars_, list):
        raise ValueError("YAML field 'variables' must be a list.")
    out: List[Dict[str, str]] = []
    for v in vars_:
        if not isinstance(v, dict) or "name" not in v:
            raise ValueError("Each item in 'variables' must be an object with at least a 'name'.")
        out.append({"name": str(v["name"])})
    return out


def _coerce_inference(y: Dict[str, Any]) -> Dict[str, Any]:
    inf = y.get("inference_config") or {}
    if not isinstance(inf, dict):
        raise ValueError("YAML field 'inference_config' must be an object.")
    # Map YAML snake_case to Bedrock Prompt Management text inference keys
    text_cfg: Dict[str, Any] = {}
    if "max_tokens" in inf:
        text_cfg["maxTokens"] = int(inf["max_tokens"])
    if "temperature" in inf:
        text_cfg["temperature"] = float(inf["temperature"])
    if "top_p" in inf:
        text_cfg["topP"] = float(inf["top_p"])
    if "top_k" in inf:
        # Bedrock prompt inference config supports topK for some models; keeping as provided.
        text_cfg["topK"] = int(inf["top_k"])
    if "stop_sequences" in inf:
        text_cfg["stopSequences"] = list(inf["stop_sequences"])
    return {"text": text_cfg} if text_cfg else {"text": {}}


def _find_prompt_identifier_by_name(agent_client, prompt_name: str) -> str:
    paginator = agent_client.get_paginator("list_prompts")
    for page in paginator.paginate():
        for s in page.get("promptSummaries", []):
            if s.get("name") == prompt_name:
                # You can use the id or the arn as promptIdentifier for UpdatePrompt.  :contentReference[oaicite:9]{index=9}
                return s.get("id") or s.get("arn")
    raise RuntimeError(f"Prompt with name '{prompt_name}' not found.")


def update_prompt_draft(
    agent_client,
    prompt_identifier: str,
    prompt_name: str,
    description: str,
    template_text: str,
    variable_defs: List[Dict[str, str]],
    model_id: str,
    variant_name: str = "default",
    enable_cachepoint: bool = False,
    kms_key_arn: Optional[str] = None,
    inference_cfg: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    template_cfg: Dict[str, Any] = {
        "text": {
            "text": template_text,
            "inputVariables": variable_defs,
        }
    }
    if enable_cachepoint:
        # CachePointBlock only requires {"type":"default"} in Prompt Management APIs. :contentReference[oaicite:10]{index=10}
        template_cfg["text"]["cachePoint"] = {"type": "default"}

    variant: Dict[str, Any] = {
        "name": variant_name,
        "modelId": model_id,
        "templateType": "TEXT",
        "templateConfiguration": template_cfg,
    }
    
    # Only include inferenceConfiguration if provided and has text config
    if inference_cfg and inference_cfg.get("text"):
        variant["inferenceConfiguration"] = inference_cfg
    
    req: Dict[str, Any] = {
        "promptIdentifier": prompt_identifier,
        "name": prompt_name,
        "description": description,
        "defaultVariant": variant_name,
        "variants": [variant],
    }

    if kms_key_arn:
        req["customerEncryptionKeyArn"] = kms_key_arn

    # UpdatePrompt updates the DRAFT version. :contentReference[oaicite:11]{index=11}
    return agent_client.update_prompt(**req)


def publish_prompt_version(agent_client, prompt_identifier: str, version_description: Optional[str] = None) -> Dict[str, Any]:
    kwargs: Dict[str, Any] = {"promptIdentifier": prompt_identifier}
    if version_description:
        kwargs["description"] = version_description
    return agent_client.create_prompt_version(**kwargs)


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--region", required=True)
    group = ap.add_mutually_exclusive_group(required=True)
    group.add_argument("--prompt-id", help="Prompt id or ARN (promptIdentifier).")
    group.add_argument("--prompt-name", help="Prompt name to look up via ListPrompts.")
    ap.add_argument("--yaml", required=True, help="Path to prompt YAML.")
    ap.add_argument("--model-id", required=True, help="Model ARN or inference profile ARN to attach to the variant.")
    ap.add_argument("--variant-name", default="default")
    ap.add_argument("--enable-cachepoint", action="store_true")
    ap.add_argument("--kms-key-arn", default=None)
    ap.add_argument("--publish-version", action="store_true")
    ap.add_argument("--version-description", default=None)
    args = ap.parse_args()

    y = _load_yaml(args.yaml)

    prompt_name = str(y.get("name") or args.prompt_name or "")
    if not prompt_name:
        raise ValueError("Prompt 'name' must be provided either in YAML or via --prompt-name.")

    description = str(y.get("description") or "")
    template_text = str(y.get("template") or "")
    if not template_text:
        raise ValueError("YAML must include 'template'.")

    variable_defs = _coerce_variables(y)

    agent_client = boto3.client("bedrock-agent", region_name=args.region)

    prompt_identifier = args.prompt_id or _find_prompt_identifier_by_name(agent_client, args.prompt_name)

    # If you want to drive inference config from YAML too, wire it here:
    inference_cfg = _coerce_inference(y)

    # Inject inference configuration (TEXT) if present.
    # (Keeping your Terraform pattern: inference config lives in prompt variant.)
    resp = update_prompt_draft(
        agent_client=agent_client,
        prompt_identifier=prompt_identifier,
        prompt_name=prompt_name,
        description=description,
        template_text=template_text,
        variable_defs=variable_defs,
        model_id=args.model_id,
        variant_name=args.variant_name,
        enable_cachepoint=args.enable_cachepoint,
        kms_key_arn=args.kms_key_arn,
        inference_cfg=inference_cfg,
    )

    print("Updated prompt DRAFT:")
    print(f"  id:      {resp.get('id')}")
    print(f"  arn:     {resp.get('arn')}")
    print(f"  version: {resp.get('version')}")  # should be DRAFT :contentReference[oaicite:13]{index=13}

    if args.publish_version:
        v = publish_prompt_version(agent_client, prompt_identifier, args.version_description)
        print("Published prompt version:")
        print(f"  version: {v.get('version')}")
        print(f"  arn:     {v.get('arn')}")

    return 0


if __name__ == "__main__":
    try:
        raise SystemExit(main())
    except Exception as e:
        print(f"ERROR: {e}", file=sys.stderr)
        raise
